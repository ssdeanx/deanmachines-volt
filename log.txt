{
  "message": "'system messages are only supported at the beginning of the conversation' functionality not supported.",
  "stage": "llm_stream",
  "originalError": "AI_UnsupportedFunctionalityError: 'system messages are only supported at the beginning of the conversation' functionality not supported."
}

````shell
### Subagents | VoltAgent

Subagents are specialized agents that work under a supervisor agent to handle specific tasks. This architecture
e enables the creation of complex agent workflows where each subagent focuses on its area of expertise, coordina
ated by a supervisor.

**Why Use Subagents?**
*   **Specialization**: Create agents that excel at specific tasks (e.g., coding, translation, data analysis).
*   **Workflow Orchestration**: Build complex, multi-step workflows by having a supervisor delegate tasks to th
he appropriate specialized agents.
*   **Scalability**: Break down complex problems into smaller, manageable parts, making the overall system easi
ier to develop and maintain.
*   **Improved Responses**: Achieve better results by leveraging the focused knowledge and capabilities of spec
cialized agents for specific parts of a user request.
*   **Modularity**: Easily swap or add specialized agents without disrupting the entire system.

**Creating and Using Subagents**

**Creating Individual Agents**
First, create the specialized agents that will serve as subagents:

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create a specialized agent for writing stories
const storyAgent = new Agent({
  name: "Story Agent",
  purpose: "A story writer agent that creates original, engaging short stories.",
  instructions: "You are a creative story writer. Create original, engaging short stories.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
});

// Create a specialized agent for translation
const translatorAgent = new Agent({
  name: "Translator Agent",
  purpose: "A translator agent that translates text accurately.",
  instructions: "You are a skilled translator. Translate text accurately.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
});
````

__Creating a Supervisor Agent__ Create a supervisor agent that will coordinate between subagents. Simply pass the specialized agents in the `su ubAgents` array during initialization:

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create a supervisor agent with specialized agents as subagents
const supervisorAgent = new Agent({
  name: "Supervisor Agent",
  instructions: "You manage a workflow between specialized agents.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
  // Specify subagents during initialization
  subAgents: [storyAgent, translatorAgent],
});
```

When you initialize an agent with subagents, several things happen automatically:

- __Supervisor Prompt Enhancement__: The supervisor agent's system prompt is automatically modified (`generat teSupervisorSystemMessage`) to include instructions on how to manage its subagents effectively. It lists the ava ailable subagents and their purpose and provides guidelines for delegation, communication, and response aggregat tion.
- __`delegate_task` Tool__: A `delegate_task` tool is automatically added to the supervisor agent's available e tools. This tool allows the supervisor LLM to decide when and how to delegate tasks.
- __Agent Registry__: The parent-child relationship between the supervisor and its subagents is registered in n the `AgentRegistry`, enabling discoverability and management within the broader system.

__How Subagents Work__ The core mechanism involves the supervisor agent delegating tasks to its subagents using the automatically prov vided `delegate_task` tool.

- A user sends a request to the supervisor agent.
- The supervisor agent's LLM analyzes the request and its enhanced system prompt (which lists available subag gents).
- Based on the task, the supervisor decides which subagent(s) are best suited to handle specific parts of the e request.
- The supervisor uses the `delegate_task` tool to hand off the task(s).

__The `delegate_task` Tool__ This tool is the primary interface for delegation.

- __Name__: `delegate_task`

- __Description__: "Delegate a task to one or more specialized agents"

- __Parameters__:

  - `task` (string, required): The specific task description to be delegated.
  - `targetAgents` (array of strings, required): A list of subagent __names__ to delegate the task to. The supervisor can delegate to multiple agents simultaneously if appropriate.
  - `context` (object, optional): Any additional context needed by the subagent(s) to perform the task.

- __Execution__:

  - The tool finds the corresponding subagent instances based on the provided names.
  - It calls the `handoffTask` (or `handoffToMultiple`) method internally, which sends the task description n and context to the target subagent(s).
  - Crucially, it passes the supervisor's agent ID (`parentAgentId`) and the current history entry ID (`par rentHistoryEntryId`) to the subagent's execution context. This is key for [Observability](#observability-and-eve ent-tracking).

- __Returns__: An array of objects, where each object contains the result from a delegated agent:

  ```json
  [
    {
      "agentName": "string",       // Name of the subagent that executed the task
      "response": "string",        // The text result returned by the subagent
      "conversationId": "string"   // The conversation ID used for the handoff
    }
    // ... potentially more results if multiple agents were targeted
  ]
  ```

- Subagents process their delegated tasks independently. They can use their own tools or even delegate furthe er if they are also supervisors.

- Each subagent returns its result to the `delegate_task` tool execution context within the supervisor.

- The supervisor receives the results from the `delegate_task` tool.

- Based on its instructions and the received results, the supervisor synthesizes the final response and prese ents it to the user.

__Complete Working Example__ Here's a full example you can copy and run to see subagents in action:

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create specialized agents
const storyAgent = new Agent({
  name: "Story Agent",
  purpose: "A story writer agent that creates original, engaging short stories.",
  instructions: "You are a creative story writer. Create original, engaging short stories.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
});

const translatorAgent = new Agent({
  name: "Translator Agent",
  purpose: "A translator agent that translates text accurately.",
  instructions: "You are a skilled translator. Translate text accurately.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
});

// Create the supervisor agent
const supervisorAgent = new Agent({
  name: "Supervisor Agent",
  instructions:
    "You manage a workflow between specialized agents. When asked for a story, " +
    "use the Story Agent to create it. Then use the Translator Agent to translate the story. " +
    "Present both versions to the user.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
  subAgents: [storyAgent, translatorAgent],
});

// Use the supervisor agent to handle a user request
const result = await supervisorAgent.streamText(
  "Write a short story about a robot learning to paint and translate it to German."
);

// Process the streamed response
for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}

/* Expected Output:
1. Supervisor analyzes the request
2. Supervisor calls delegate_task tool → Story Agent
3. Story Agent creates the story
4. Supervisor calls delegate_task tool → Translator Agent
5. Translator Agent translates to German
6. Supervisor presents both versions
Final response includes both the original story and German translation.
*/
```

__Combining with Hooks__ You can use [hooks](#hooks), specifically `onHandoff`, to inject custom logic when a task is delegated from a s supervisor to a subagent via the `delegate_task` tool.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create a supervisor agent with hooks for monitoring subagent interactions
const supervisorAgent = new Agent({
  name: "Supervisor Agent",
  instructions: "You manage a workflow between specialized agents.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
  subAgents: [storyAgent, translatorAgent],
  hooks: {
    onHandoff: async (targetAgent, sourceAgent) => {
      // 'sourceAgent' is the supervisor, 'targetAgent' is the subagent receiving the task
      console.log(`Task being handed off from ${sourceAgent.name} to ${targetAgent.name}`);
      // --- Use Cases ---
      // 1. Logging: Log detailed information about the handoff for debugging/monitoring.
      // 2. Validation: Check if the handoff is appropriate based on agent capabilities or context.
      // 3. Context Modification: Potentially modify the context being passed (though direct modification isn't
t standard, you could trigger external updates).
      // 4. Notification: Send notifications about task delegation.
    },
  },
});
```

The `onHandoff` hook is triggered within the `handoffTask` method just before the target agent starts processin ng the delegated task.

__Context Sharing Between Agents__ SubAgents automatically inherit the supervisor's operation context, including `userContext` and conversation hi istory. This enables seamless data sharing across the agent hierarchy.

__Automatic Context Inheritance__ When a supervisor delegates a task, the subagent receives:

- __`userContext`__: All custom data from the supervisor's operation
- __`conversationSteps`__: Shared conversation history (steps are added to the same array)
- __`parentAgentId`__: Reference to the supervisor for traceability

```typescript
// Supervisor sets context
const response = await supervisorAgent.streamText("Translate this story", {
  userContext: new Map([
    ["projectId", "proj-123"],
    ["language", "Spanish"],
    ["priority", "high"],
  ]),
});

// SubAgent automatically receives this context and can access it in hooks/tools
const translatorAgent = new Agent({
  name: "Translator Agent",
  hooks: createHooks({
    onStart: ({ context }) => {
      // Access supervisor's context
      const projectId = context.userContext.get("projectId");
      const language = context.userContext.get("language");
      console.log(`Translating for project ${projectId} to ${language}`);
    },
  }),
  // ... other config
});
```

__Shared Conversation History__ SubAgents contribute to the same conversation history as their supervisor, making the entire workflow appear as s one cohesive operation:

```typescript
const supervisorAgent = new Agent({
  name: "Supervisor",
  subAgents: [translatorAgent, reviewerAgent],
  hooks: createHooks({
    onEnd: ({ context }) => {
      // Access all steps from supervisor AND subagents
      const allSteps = context.conversationSteps;
      console.log(`Total workflow steps: ${allSteps.length}`);
      // Steps include:
      // - Supervisor's tool calls (delegate_task)
      // - SubAgent's processing steps
      // - All tool executions across agents
      // This creates a complete audit trail
    },
  }),
  instructions: "Coordinate translation and review workflow",
});

// When you call the supervisor, you get a unified history
const response = await supervisorAgent.streamText("Translate and review this text");
// response.userContext contains the complete workflow state
```

For detailed examples and patterns, see the Operation Context guide.

__Observability and Event Tracking__ To maintain traceability in complex workflows involving multiple agents, the system automatically propagates co ontext during task handoffs:

- When the supervisor's `delegate_task` tool calls a subagent (via `handoffTask`), it includes the supervisor r's `agentId` as `parentAgentId` and the supervisor's current `historyEntryId` as `parentHistoryEntryId` in the subagent's operation context.
- This means any events (like tool calls, LLM steps, errors) generated by the subagent while processing the d delegated task will be associated not only with the subagent's own history entry but also linked back to the ori iginal history entry in the supervisor agent.
- This allows monitoring and debugging tools to reconstruct the entire flow of execution across multiple agen nts for a single user request.

__Adding Subagents After Initialization__ You can also add subagents after creating the supervisor agent:

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create a new specialized agent
const factCheckerAgent = new Agent({
  name: "Fact Checker Agent",
  purpose: "A fact checker agent that verifies facts and provides accurate information.",
  instructions: "You verify facts and provide accurate information.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o-mini"),
});

// Add the agent as a subagent to the supervisor
// This also registers the relationship in AgentRegistry
supervisorAgent.addSubAgent(factCheckerAgent);
```

__Removing Subagents__

```typescript
// Remove a subagent by its ID
// This also unregisters the relationship in AgentRegistry
supervisorAgent.removeSubAgent(factCheckerAgent.id);
```

__Troubleshooting__

__SubAgent Not Being Called?__

- __Check Agent Names__: The `delegate_task` tool uses agent names, not IDs:

  ```typescript
  // ✅ Correct
  const subAgent = new Agent({ name: "Story Agent", ... });

  // ❌ Wrong - LLM will try to call "story-agent-id"
  const subAgent = new Agent({ name: "story-agent-id", ... });
  ```

- __Improve Supervisor Instructions__: Be explicit about when to delegate:

  ```typescript
  const supervisor = new Agent({
    instructions: `
      You coordinate specialized agents:
      - For creative writing: use Story Agent
      - For translation: use Translator Agent

      Always delegate tasks to the appropriate specialist.
    `,
    // ...
  });
  ```

- __Debug Context Passing__: Check if context is being inherited:

  ```typescript
  const subAgent = new Agent({
    hooks: createHooks({
      onStart: ({ context }) => {
        console.log("SubAgent context:", Object.fromEntries(context.userContext));
      },
    }),
    // ...
  });
  ```

__Monitor the Workflow__ Enable logging to see the delegation flow:

```typescript
const supervisor = new Agent({
  hooks: createHooks({
    onToolStart: ({ tool }) => {
      if (tool.name === "delegate_task") {
        console.log("🔄 Delegating task to subagent");
      }
    },
  }),
  // ...
});
```

---

### Hooks | VoltAgent

Hooks provide points within the agent's execution lifecycle where you can inject custom logic. They allow you t to run code before an agent starts processing, after it finishes, before and after it uses a tool, or when tasks s are handed off between agents in a multi-agent system.

This is useful for logging, monitoring, adding validation, managing resources, or modifying behavior.

__Creating and Using Hooks__ The recommended way to define hooks is using the `createHooks` helper function. This creates a typed hooks obje ect that can be passed to one or more agents during initialization.

```typescript
import {
  Agent,
  createHooks,
  type AgentTool,
  type AgentOperationOutput, // Unified success output type
  type VoltAgentError, // Standardized error type
  type ChatMessage, // Vercel AI SDK compatible message format
  type OnStartHookArgs, // Argument types for hooks
  type OnEndHookArgs,
  type OnToolStartHookArgs,
  type OnToolEndHookArgs,
  type OnHandoffHookArgs,
} from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Define a collection of hooks using the helper
const myAgentHooks = createHooks({
  /**
   * Called before the agent starts processing a request.
   */
  onStart: async (args: OnStartHookArgs) => {
    const { agent, context } = args;
    console.log(`[Hook] Agent ${agent.name} starting interaction at ${new Date().toISOString()}`);
    console.log(`[Hook] Operation ID: ${context.operationId}`);
  },
  /**
   * Called after the agent finishes processing a request, successfully or with an error.
   */
  onEnd: async (args: OnEndHookArgs) => {
    const { agent, output, error, messages, context } = args;
    if (error) {
      console.error(`[Hook] Agent ${agent.name} finished with error:`, error.message);
      console.log(`[Hook] User input was:`, messages[0]?.content);
      // Log detailed error info
      console.error(`[Hook] Error Details:`, JSON.stringify(error, null, 2));
    } else if (output) {
      console.log(`[Hook] Agent ${agent.name} finished successfully.`);
      console.log(`[Hook] Conversation turn:`, {
        userInput: messages[0]?.content,
        assistantResponse: messages[1]?.content,
      });
      // Example: Log usage or analyze the result based on output type
      if ("usage" in output && output.usage) {
        console.log(`[Hook] Token Usage: ${output.usage.totalTokens}`);
      }
      if ("text" in output && output.text) {
        console.log(`[Hook] Final text length: ${output.text.length}`);
      }
      if ("object" in output && output.object) {
        console.log(`[Hook] Final object keys: ${Object.keys(output.object).join(", ")}`);
      }
    }
  },
  /**
   * Called just before a tool's execute function is called.
   */
  onToolStart: async (args: OnToolStartHookArgs) => {
    const { agent, tool, context } = args;
    console.log(`[Hook] Agent ${agent.name} starting tool: ${tool.name}`);
    // Example: Validate tool inputs or log intent
  },
  /**
   * Called after a tool's execute function completes or throws.
   */
  onToolEnd: async (args: OnToolEndHookArgs) => {
    const { agent, tool, output, error, context } = args;
    if (error) {
      console.error(`[Hook] Tool ${tool.name} failed with error:`, error.message);
      // Log detailed tool error
      console.error(`[Hook] Tool Error Details:`, JSON.stringify(error, null, 2));
    } else {
      console.log(`[Hook] Tool ${tool.name} completed successfully with result:`, output);
      // Example: Log tool output or trigger follow-up actions
    }
  },
  /**
   * Called when a task is handed off from a source agent to this agent (in sub-agent scenarios).
   */
  onHandoff: async (args: OnHandoffHookArgs) => {
    const { agent, sourceAgent } = args;
    console.log(`[Hook] Task handed off from ${sourceAgent.name} to ${agent.name}`);
    // Example: Track collaboration flow in multi-agent systems
  },
});

// Define a placeholder provider for the example
const provider = new VercelAIProvider();

// Assign the hooks when creating an agent
const agentWithHooks = new Agent({
  name: "My Agent with Hooks",
  instructions: "An assistant demonstrating hooks",
  llm: provider,
  model: openai("gpt-4o"),
  // Pass the hooks object during initialization
  hooks: myAgentHooks,
});

// Alternatively, define hooks inline (less reusable)
const agentWithInlineHooks = new Agent({
  name: "Inline Hooks Agent",
  instructions: "Another assistant",
  llm: provider,
  model: openai("gpt-4o"),
  hooks: {
    onStart: async ({ agent, context }) => {
      // Use object destructuring
      /* ... */
    },
    onEnd: async ({ agent, output, error, context }) => {
      /* ... */
    },
    // ... other inline hooks ...
  },
});
```

__Available Hooks__ All hooks receive a single argument object containing relevant information.

__onStart__

- __Triggered:__ Before the agent begins processing a request (`generateText`, `streamText`, etc.).
- __Argument Object (`OnStartHookArgs`):__ `{ agent: Agent, context: OperationContext }`
- __Use Cases:__ Initialization logic, request logging, setting up request-scoped resources.
  ```typescript
  // Example: Log the start of an operation
  onStart: async ({ agent, context }) => {
    console.log(`Agent ${agent.name} starting operation ${context.operationId}`);
  };
  ```

__onEnd__

- __Triggered:__ After the agent finishes processing a request, either successfully or with an error.
- __Argument Object (`OnEndHookArgs`):__ `{ agent: Agent, output: AgentOperationOutput | undefined, error: Vo oltAgentError | undefined, conversationId: string, context: OperationContext }`
- __Use Cases:__ Cleanup logic, logging completion status and results (success or failure), analyzing final o output or error details, recording usage statistics, storing conversation history.
- __Note:__ The output object's specific structure within the `AgentOperationOutput` union depends on the age ent method called. Check for specific fields (`text`, `object`) or use type guards. `error` will contain the str ructured `VoltAgentError` on failure.
  ```typescript
  // Example: Log the outcome of an operation and store conversation history
  onEnd: async ({ agent, output, error, conversationId, context }) => {
    if (error) {
      console.error(`Agent ${agent.name} operation ${context.operationId} failed: ${error.message}`);
      console.log(`User input: "${context.historyEntry.input}"`);
      // Only user input available on error (no assistant response)
    } else {
      // Check output type if needed
      if (output && "text" in output) {
        console.log(
          `Agent ${agent.name} operation ${context.operationId} succeeded with text output.`
        );
      } else if (output && "object" in output) {
        console.log(
          `Agent ${agent.name} operation ${context.operationId} succeeded with object output.`
        );
      } else {
        console.log(`Agent ${agent.name} operation ${context.operationId} succeeded.`);
      }
      // Log the complete conversation flow
      console.log(`Conversation flow:`, {
        user: context.historyEntry.input,
        assistant: context.steps, // the assistant steps
        totalMessages: context.steps.length,
        toolInteractions: context.steps.flatMap((s) => s.toolInvocations || []).length,
        toolsUsed: context.steps.flatMap((s) => s.toolInvocations || []).map((t) => t.toolName),
      });
      // Log usage if available
      if (output?.usage) {
        console.log(`  Usage: ${output.usage.totalTokens} tokens`);
      }
    }
  };
  ```

__onToolStart__

- __Triggered:__ Just before an agent executes a specific tool.
- __Argument Object (`OnToolStartHookArgs`):__ `{ agent: Agent, tool: AgentTool, context: OperationContext }`
- __Use Cases:__ Logging tool usage intent, validating tool inputs (though typically handled by Zod schema), modifying tool arguments (use with caution).
  ```typescript
  // Example: Log when a tool is about to be used
  onToolStart: async ({ agent, tool, context }) => {
    console.log(
      `Agent ${agent.name} invoking tool '${tool.name}' for operation ${context.operationId}`
    );
  };
  ```

__onToolEnd__

- __Triggered:__ After a tool's execute function successfully completes or fails.
- __Argument Object (`OnToolEndHookArgs`):__ `{ agent: Agent, tool: AgentTool, output: unknown | undefined, e error: VoltAgentError | undefined, context: OperationContext }`
- __Use Cases:__ Logging tool results or errors, post-processing tool output, triggering subsequent actions b based on tool success or failure.
  ```typescript
  // Example: Log the result or error of a tool execution
  onToolEnd: async ({ agent, tool, output, error, context }) => {
    if (error) {
      console.error(
        `Tool '${tool.name}' failed in operation ${context.operationId}: ${error.message}`
      );
    } else {
      console.log(
        `Tool '${tool.name}' succeeded in operation ${context.operationId}. Result:`,
        output
      );
    }
  };
  ```

__onHandoff__

- __Triggered:__ When one agent delegates a task to another agent (using the `delegate_task` tool in a sub-ag gent setup).
- __Argument Object (`OnHandoffHookArgs`):__ `{ agent: Agent, sourceAgent: Agent }`
- __Use Cases:__ Tracking and visualizing workflow in multi-agent systems, adding context during agent collab boration.
  ```typescript
  // Example: Log agent handoffs
  onHandoff: async ({ agent, sourceAgent }) => {
    console.log(`Task handed off from agent '${sourceAgent.name}' to agent '${agent.name}'`);
  };
  ```

__Asynchronous Hooks and Error Handling__

- __Async Nature:__ Hooks can be defined as async functions. VoltAgent will await the completion of each hook k before proceeding. Be mindful that long-running asynchronous operations within hooks can add latency to the ov verall agent response time.
- __Error Handling:__ If an error is thrown *inside* a hook function and not caught within the hook itself, i it may interrupt the agent's execution flow. It's recommended to handle potential errors within your hook logic using try...catch if necessary, or ensure hooks are designed to be reliable.

__Common Use Cases__ Hooks enable a variety of powerful patterns:

- __Logging & Observability__: Track agent execution steps, timings, inputs, outputs, and errors for monitori ing and debugging.
- __Analytics__: Collect detailed usage data (token counts, tool usage frequency, success/error rates) for an nalysis.
- __Request/Response Modification__: (Use with caution) Modify inputs before processing or outputs after gene eration.
- __State Management__: Initialize or clean up request-specific state or resources.
- __Workflow Orchestration__: Trigger external actions or notifications based on agent events (e.g., notify o on tool failure or successful completion with specific output).
- __UI Integration__: You can leverage the `@voltagent/vercel-ui` package to convert the `OperationContext` t to a list of messages that can be used with the Vercel AI SDK.

__Examples__

__Vercel UI Integration Example__ Here's an example of how you can use the `@voltagent/vercel-ui` package to convert the `OperationContext` to a list of messages that can be used with the Vercel AI SDK:

```typescript
import { convertToUIMessages } from "@voltagent/vercel-ui";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { Agent } from "@voltagent/core";

const agent = new Agent({
  id: "assistant",
  name: "Assistant",
  purpose: "A helpful assistant that can answer questions and help with tasks.",
  instructions: "You are a helpful assistant that can answer questions and help with tasks.",
  model: "gpt-4.1-mini",
  llm: new VercelAIProvider(),
  hooks: {
    onEnd: async (result) => {
      await chatStore.save({
        conversationId: result.conversationId,
        messages: convertToUIMessages(result.operationContext),
      });
    },
  },
});

const result = await agent.generateText("Hello, how are you?");

// You can now fetch the messages from your custom chat store and return to the UI to provide a
// history of the conversation.
app.get("/api/chats/:id", async ({ req }) => {
  const conversation = await chatStore.read(req.param("id"));
  return Response.json(conversation.messages);
});
```

__Full Conversation Flow Example__ Here's an example showing how the messages parameter includes complete conversation flow with tool interactions s:

```typescript
const conversationHooks = createHooks({
  onEnd: async ({ agent, output, error, messages, context }) => {
    // Example messages array for a successful operation with tool usage (ChatMessage format):
    // [
    //   {
    //     id: "msg_1",
    //     role: "user",
    //     content: "What's the weather in San Francisco?",
    //     createdAt: new Date()
    //   },
    //   {
    //     id: "msg_2",
    //     role: "assistant",
    //     content: "The weather in San Francisco is 8°C and rainy with 86% humidity.",
    //     createdAt: new Date(),
    //     toolInvocations: [
    //       {
    //         toolCallId: "call_mmZhyZwnheCjZQCRxFPR14pF",
    //         toolName: "getWeather",
    //         args: { location: "San Francisco" },
    //         result: {
    //           weather: { location: "San Francisco", temperature: 8, condition: "Rainy", humidity: 86, windSp
peed: 14 },
    //           message: "Current weather in San Francisco: 8°C and rainy with 86% humidity."
    //         },
    //         state: "result",
    //         step: 0
    //       }
    //     ]
    //   }
    // ]
    if (!error && output) {
      // Store complete conversation including tool interactions
      await storeConversation({
        operationId: context.operationId,
        messages: messages, // Full conversation flow
        usage: output.usage,
        timestamp: new Date(),
      });

      // Check if tools were used
      const toolInteractions = messages.flatMap((m) => m.toolInvocations || []);
      if (toolInteractions.length > 0) {
        console.log(`Operation used ${toolInteractions.length} tool(s)`);
        toolInteractions.forEach((tool, i) => {
          console.log(`  Tool ${i + 1}: ${tool.toolName} (${tool.state})`);
        });
      }
    }
  },
});
```

---

### Agent Overview | VoltAgent

The `Agent` class is the fundamental building block of VoltAgent. It acts as the central orchestrator, allowing g you to create AI agents that interact with Large Language Models (LLMs), use tools to interact with the outsid de world, maintain conversational memory, and embody specific personalities or instructions.

__Creating an Agent__ At its core, an agent needs a name, instructions (which guides its behavior), an LLM Provider to handle communi ication with an AI model, and the specific model to use.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai"; // Handles communication
import { openai } from "@ai-sdk/openai"; // Defines the specific model source

const agent = new Agent({
  name: "My Assistant",
  instructions: "A helpful and friendly assistant that can answer questions clearly and concisely.",
  // The LLM Provider acts as the bridge to the AI service
  llm: new VercelAIProvider(),
  // The model specifies which AI model to use (e.g., from OpenAI via Vercel AI SDK)
  model: openai("gpt-4o"),
});
```

__Core Interaction Methods__ The primary ways to interact with an agent are through the `generate*` and `stream*` methods. These methods han ndle sending your input to the configured LLM, processing the response, and potentially orchestrating tool usage e or memory retrieval based on the agent's configuration and the LLM's decisions.

__Text Generation (generateText/streamText)__ Use these methods when you expect a primarily text-based response. The agent might still decide to use tools ba ased on the prompt and its capabilities.

- `generateText`: Returns the complete text response after the LLM and any necessary tool calls are finished.
- `streamText`: Returns a stream that yields chunks of the response (text, tool calls, tool results) as they become available, providing a more interactive experience.

```typescript
import { Agent, createTool } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

// Example Tool (see Tools section for details)
const weatherTool = createTool({
  name: "get_weather",
  description: "Get the current weather for a specific location",
  parameters: z.object({ location: z.string().describe("City and state") }),
  execute: async ({ location }) => {
    console.log(`Tool: Getting weather for ${location}`);
    // Call API... return mock data
    return { temperature: 72, conditions: "sunny" };
  },
});

const agent = new Agent({
  name: "Chat Assistant",
  instructions: "A helpful assistant that can check the weather.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  tools: [weatherTool],
});

// Example using streamText for a chat-like interaction
async function chat(input: string) {
  console.log(`User: ${input}`);
  // Use streamText for interactive responses
  const stream = await agent.streamText(input);
  for await (const chunk of stream.textStream) {
    console.log(chunk);
  }
}

// Example usage that might trigger the weather tool
await chat("What's the weather like in London?");

// Example using generateText for a complete response
const completeResponse = await agent.generateText("Explain machine learning briefly.");
console.log("Complete Response:", completeResponse.text);
```

__Enhanced Streaming with `fullStream`__ For more detailed streaming information including tool calls, reasoning steps, and completion status, you can u use the `fullStream` property available in the response:

```typescript
// Example using fullStream for detailed streaming events
async function enhancedChat(input: string) {
  console.log(`User: ${input}`);
  const response = await agent.streamText(input);

  // Check if fullStream is available (provider-dependent)
  if (response.fullStream) {
    for await (const chunk of response.fullStream) {
      switch (chunk.type) {
        case "text-delta":
          // Output text as it's generated
          process.stdout.write(chunk.textDelta);
          break;
        case "tool-call":
          console.log(`\n🔧 Using tool: ${chunk.toolName}`);
          break;
        case "tool-result":
          console.log(`✅ Tool completed: ${chunk.toolName}`);
          break;
        case "reasoning":
          console.log(`🤔 AI thinking: ${chunk.reasoning}`);
          break;
        case "source":
          console.log(`📚 Retrieved context: ${chunk.source}`);
          break;
        case "finish":
          console.log(`\n✨ Done! Tokens used: ${chunk.usage?.totalTokens}`);
          break;
      }
    }
  } else {
    // Fallback to standard textStream
    for await (const chunk of response.textStream) {
      process.stdout.write(chunk);
    }
  }
}

await enhancedChat("Write a short story about a cat and format it nicely");
```

`fullStream` Support Currently, `fullStream` is only supported by the `@voltagent/vercel-ai` provider. For other providers (Google A AI, Groq, Anthropic, XsAI), the response will fall back to the standard `textStream`. We're actively looking for r community contributions to add `fullStream` support to other providers! If you're interested in helping, pleas se check out our GitHub repository or join our Discord community.

SubAgent Event Filtering When using `fullStream` with sub-agents, all sub-agent events are automatically forwarded to the parent stream with `subAgentId` and `subAgentName` metadata. You can filter these events on the client side for different UI experiences:

```typescript
const response = await supervisorAgent.streamText("Write a story and format it");
if (response.fullStream) {
  for await (const chunk of response.fullStream) {
    const isSubAgentEvent = chunk.subAgentId && chunk.subAgentName;

    if (isSubAgentEvent) {
      // Option 1: Skip all SubAgent events for a clean UI
      continue;

      // Option 2: Show only SubAgent tool activities
      if (chunk.type === "tool-call" || chunk.type === "tool-result") {
        console.log(`[${chunk.subAgentName}] Tool: ${chunk.toolName}`);
      }
      continue;

      // Option 3: Show all SubAgent events with labels
      console.log(`[${chunk.subAgentName}] ${chunk.type}:`, chunk);
    } else {
      // Process main supervisor events
      handleMainAgentEvent(chunk);
    }
  }
}
```

__Available SubAgent Event Types:__

- `text-delta`: SubAgent text output (character by character)
- `reasoning`: SubAgent internal reasoning steps
- `source`: SubAgent context retrieval results
- `tool-call`: SubAgent tool execution starts
- `tool-result`: SubAgent tool execution completes This filtering approach allows you to create different UI experiences while preserving all events for debugging g and monitoring.

__Markdown Formatting__ __Why?__ To have the agent automatically format its text responses using Markdown for better readability and pr resentation.

By setting the `markdown` property to `true` in the agent's configuration, you instruct the LLM to use Markdown n syntax (like headings, lists, bold text, etc.) when generating text responses. VoltAgent adds a corresponding instruction to the system prompt automatically.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  name: "Markdown Assistant",
  instructions: "A helpful assistant that formats answers clearly.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  markdown: true, // Enable automatic Markdown formatting
});

// Now, when you call generateText or streamText,
// the agent will attempt to format its response using Markdown.
const response = await agent.generateText("Explain the steps to make a cup of tea.");
console.log(response.text);
```

This is particularly useful when displaying agent responses in UIs that support Markdown rendering.

__Structured Data Generation (generateObject/streamObject)__ Use these methods when you need the LLM to generate output conforming to a specific structure (defined by a Zod d schema). This is ideal for data extraction, function calling based on schema, or generating predictable JSON.

- `generateObject`: Returns the complete structured object once generation is finished.
- `streamObject`: Returns a stream that yields partial updates to the object as it's being constructed by the e LLM.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

const agent = new Agent({
  name: "Data Extractor",
  instructions: "Extracts structured data.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"), // Ensure model supports structured output/function calling
});

// Define a simple schema with Zod
const personSchema = z.object({
  name: z.string().describe("Full name"), // Descriptions help the LLM
  age: z.number(),
  occupation: z.string(),
  skills: z.array(z.string()),
});

// Example using generateObject
const objectResponse = await agent.generateObject(
  "Create a profile for a talented software developer named Alex.",
  personSchema
);
console.log("Complete object:", objectResponse.object);

// Example using streamObject
const streamObjectResponse = await agent.streamObject(
  "Generate details for a data scientist named Jamie.",
  personSchema
);
for await (const partial of streamObjectResponse.objectStream) {
  console.log("Received update:", partial); // Shows the object being built incrementally
}
const finalObject = await streamObjectResponse.object;
console.log("Final streamed object:", finalObject);
```

__Advanced Features__ Enhance your agents with these powerful capabilities, which are integrated into the core `generate*/stream*` me ethods:

__Memory__ __Why?__ To give your agent context of past interactions, enabling more natural, coherent, and personalized con nversations.

VoltAgent's memory management system allows agents to store and retrieve conversation history or state using co onfigurable Memory Providers.

```typescript
// Example: Configuring memory (Provider details omitted for brevity)
import { Agent, LibSQLStorage } from "@voltagent/core";

// ... other imports

const memoryStorage = new LibSQLStorage({
  /* ... provider config ... */
});

const agent = new Agent({
  name: "Assistant with Memory",
  // ... other config ...
  memory: memoryStorage,
});
```

When memory is configured, the agent automatically retrieves relevant context before calling the LLM and saves new interactions afterwards.

__Tools__ __Why?__ To allow your agent to interact with the outside world, access real-time information, or perform actio ons via APIs, databases, or other systems.

When you call `generateText` or `streamText`, the LLM can decide to use one of the provided tools. VoltAgent ha andles the execution and feeds the result back to the LLM to continue generation.

```typescript
import { Agent, createTool } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

// Create a weather tool using the helper function
const weatherTool = createTool({
  name: "get_weather",
  description: "Get the current weather for a specific location",
  parameters: z.object({
    location: z.string().describe("The city and state, e.g., San Francisco, CA"),
  }),
  // The function the agent executes when using the tool
  execute: async ({ location }) => {
    console.log(`Tool: Getting weather for ${location}`);
    // In a real scenario, call a weather API here
    // Returning mock data for demonstration
    if (location.toLowerCase().includes("london")) {
      return { temperature: 55, conditions: "cloudy" };
    }
    return { temperature: 72, conditions: "sunny" };
  },
});

const agent = new Agent({
  name: "Weather Assistant",
  instructions: "An assistant that can check the weather using available tools.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"), // Models supporting tool use are required
  tools: [weatherTool], // Provide the list of tools to the agent
});

// Example: Call streamText and the agent might use the tool
const response = await agent.generateText("What's the weather in London?");
console.log(response.text);
// The agent should call the 'get_weather' tool during the generation.
```

__Sub-Agents__ __Why?__ To break down complex tasks into smaller, manageable parts handled by specialized agents, promoting mo odularity and focused expertise (similar to a team of specialists).

A coordinator agent uses a special `delegate_task` tool (added automatically when sub-agents are present) to pa ass control to a sub-agent during a `generate*/stream*` call.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Assume researchAgent and writingAgent are configured Agents
const researchAgent = new Agent({ name: "Researcher" /* ... */ });
const writingAgent = new Agent({ name: "Writer" /* ... */ });

// Create a coordinator agent that uses the others
const mainAgent = new Agent({
  name: "Coordinator",
  instructions: "Coordinates research and writing tasks by delegating to specialized sub-agents.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  // List the agents this one can delegate tasks to
  subAgents: [researchAgent, writingAgent],
});

// Example: Call streamText on the main agent
const response = await mainAgent.generateText("Write a blog post about quantum computing.");
console.log(response.text);
// The Coordinator might decide to use the delegate_task tool to involve researchAgent and writingAgent.
```

__Hooks__ __Why?__ To observe and potentially intercept or modify the agent's behavior at various lifecycle stages (start t, end, tool calls, etc.) for logging, debugging, or custom logic.

Hooks are triggered at specific points during the execution of `generate*/stream*` methods. Each hook receives a single argument object containing relevant information like the agent instance and operation context.

```typescript
import {
  Agent,
  createHooks,
  type OnStartHookArgs,
  type OnEndHookArgs,
  type OnToolStartHookArgs,
  type OnToolEndHookArgs,
} from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

const hooks = createHooks({
  // Called when any agent interaction starts (generateText, streamText, etc.)
  onStart: async ({ agent, context }: OnStartHookArgs) => {
    console.log(`Agent ${agent.name} starting interaction... Context:`, context);
  },
  // Called when the interaction finishes (successfully or with an error)
  onEnd: async ({ agent, output, error, context }: OnEndHookArgs) => {
    if (error) {
      console.error(`Agent ${agent.name} finished with error:`, error);
    } else if (output) {
      // Output format depends on the method called (e.g., { text: ..., usage: ... } for generateText)
      console.log(
        `Agent ${agent.name} finished successfully. Final output:`,
        output.text ?? output.object // Access 'text' or 'object' based on the operation type
      );
    }
    console.log("Finished context:", context);
  },
  // Called before a tool is executed
  onToolStart: async ({ agent, tool, context }: OnToolStartHookArgs) => {
    console.log(`Agent ${agent.name} starting tool: ${tool.name}. Context:`, context);
  },
  // Called after a tool finishes execution (successfully or with an error)
  onToolEnd: async ({ agent, tool, output, error, context }: OnToolEndHookArgs) => {
    if (error) {
      console.error(`Agent ${agent.name} failed tool: ${tool.name}. Error:`, error);
    } else {
      console.log(
        `Agent ${agent.name} finished tool: ${tool.name}. Result:`,
        output // Tool output is directly available
      );
    }
    console.log("Tool context:", context);
  },
  // Note: There is no top-level 'onError' hook. Errors are handled within onEnd and onToolEnd.
  // The 'onHandoff' hook (not shown here) is called when control is passed between agents (e.g., sub-agents).
});

const agent = new Agent({
  name: "Observable Agent",
  instructions: "An agent with logging hooks.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  hooks, // Attach the defined hooks
});

await agent.generateText("Log this message: 'Processing user data.'");
// The requestId set in onStart will be available in loggerTool and onEnd.
```

__Dynamic Agents__ __Why?__ To create adaptive AI agents that change their behavior, capabilities, and configuration based on runt time context. Instead of having fixed instructions, models, or tools, you can define functions that dynamically determine these properties based on user context, request parameters, or any other runtime information.

Dynamic agents are perfect for multi-tenant applications, role-based access control, subscription tiers, intern nationalization, and A/B testing scenarios.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

const dynamicAgent = new Agent({
  name: "Adaptive Assistant",
  // Dynamic instructions based on user context
  instructions: ({ userContext }) => {
    const role = (userContext.get("role") as string) || "user";
    const language = (userContext.get("language") as string) || "English";
    if (role === "admin") {
      return `You are an admin assistant with special privileges. Respond in ${language}.`;
    } else {
      return `You are a helpful assistant. Respond in ${language}.`;
    }
  },
  // Dynamic model based on subscription tier
  model: ({ userContext }) => {
    const tier = (userContext.get("tier") as string) || "free";
    switch (tier) {
      case "premium":
        return openai("gpt-4o");
      case "pro":
        return openai("gpt-4o-mini");
      default:
        return openai("gpt-3.5-turbo");
    }
  },
  llm: new VercelAIProvider(),
});

// Use with context
const userContext = new Map<string, unknown>();
userContext.set("role", "admin");
userContext.set("language", "Spanish");
userContext.set("tier", "premium");

const response = await dynamicAgent.generateText("Help me manage the system settings", {
  userContext: userContext,
});
// The agent will respond in Spanish, with admin capabilities, using the premium model
```

__Operation Context (userContext)__ __Why?__ To pass custom, request-specific data between different parts of an agent's execution flow (like hooks s and tools) for a single operation, without affecting other concurrent or subsequent operations. Useful for tra acing, logging, metrics, or passing temporary configuration.

`userContext` is a `Map` accessible via the `OperationContext` object, which is passed to hooks and available i in tool execution contexts. This context is isolated to each individual operation (`generateText`, `streamObject t`, etc.).

```typescript
import {
  Agent,
  createHooks,
  createTool,
  type OperationContext,
  type ToolExecutionContext,
} from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

const hooks = createHooks({
  onStart: async (agent: Agent<any>, context: OperationContext) => {
    const requestId = `req-${Date.now()}`;
    context.userContext.set("requestId", requestId); // Set data in context
    console.log(`[${agent.name}] Operation started. RequestID: ${requestId}`);
  },
  onEnd: async (agent: Agent<any>, result: any, context: OperationContext) => {
    const requestId = context.userContext.get("requestId"); // Get data from context
    console.log(`[${agent.name}] Operation finished. RequestID: ${requestId}`);
  },
});

const loggerTool = createTool({
  name: "context_aware_logger",
  description: "Logs a message using the request ID from context.",
  parameters: z.object({ message: z.string() }),
  execute: async (params: { message: string }, options?: ToolExecutionContext) => {
    const requestId = options?.operationContext?.userContext?.get("requestId") || "unknown";
    const logMessage = `[ReqID: ${requestId}] Tool Log: ${params.message}`;
    console.log(logMessage);
    return `Logged: ${params.message}`;
  },
});

const agent = new Agent({
  name: "Context Agent",
  instructions: "Uses userContext.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  hooks: hooks,
  tools: [loggerTool],
});

await agent.generateText("Log this message: 'Processing user data.'");
// The requestId set in onStart will be available in loggerTool and onEnd.
```

__Retriever__ __Why?__ To provide the agent with access to external knowledge bases or documents, allowing it to answer quest tions or generate content based on information not present in its original training data (Retrieval-Augmented Ge eneration - RAG).

The retriever is automatically invoked before calling the LLM within `generate*/stream*` methods to fetch relev vant context, which is then added to the system prompt.

```typescript
import { BaseRetriever } from "@voltagent/core";
import type { BaseMessage } from "@voltagent/core";
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Create a simple retriever (replace with actual vector search in production)
class SimpleRetriever extends BaseRetriever {
  // Sample knowledge base
  private documents = [
    { id: "doc1", content: "VoltAgent is a TypeScript framework for building AI agents." },
    { id: "doc2", content: "Agents can use tools, memory, and sub-agents." },
    { id: "doc3", content: "Retrievers enhance AI agents with external knowledge using RAG." },
  ];

  // Method to fetch relevant documents
  async retrieve(input: string | BaseMessage[]): Promise<string> {
    // Extract the query text
    const query = typeof input === "string" ? input : (input[input.length - 1].content as string);
    console.log(`Retriever: Searching for "${query}"`);

    // Simple keyword matching (use vector embeddings for real applications)
    const results = this.documents.filter((doc) =>
      doc.content.toLowerCase().includes(query.toLowerCase())
    );

    if (results.length === 0) return "No relevant information found in documents.";

    // Format results for the LLM
    return results.map((doc) => `Document ${doc.id}: ${doc.content}`).join("\n\n");
  }
}

// Create agent with the retriever
const agent = new Agent({
  name: "Knowledge Assistant",
  instructions: "An assistant that uses retrieved documents to answer questions.",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  retriever: new SimpleRetriever(), // Add the retriever instance
});

// Example: Ask a question using streamText
const response = await agent.generateText("What are Retrievers in VoltAgent?");
console.log(response.text);
// The agent will use SimpleRetriever *before* calling the LLM,
// then generate an answer based on the retrieved context.
```

__Providers__ __Why?__ To abstract the communication layer with different LLM backends (like OpenAI, Anthropic, Google Gemini i, Cohere, local models via Ollama, etc.), allowing you to switch providers without rewriting your core agent lo ogic.

VoltAgent achieves this through `LLMProvider` implementations. You configure your Agent with a specific provide er instance and the desired model compatible with that provider.

Currently, VoltAgent offers built-in providers for various services and APIs:

- __`@voltagent/vercel-ai`__: Uses the Vercel AI SDK to connect to a wide range of models (OpenAI, Anthropic, , Google, Groq, etc.).
- __`@voltagent/xsai`__: Connects to any OpenAI-compatible API (OpenAI, Groq, Together AI, local models, etc. .).
- __`@voltagent/google-ai`__: Uses the official Google AI SDK for Gemini and Vertex AI.
- __`@voltagent/groq-ai`__: Connects specifically to the Groq API for fast inference.
- __`@voltagent/anthropic-ai`__: Connects directly to Anthropic's AI models (Claude) using the official `anth hropic-ai/sdk` SDK.

We plan to add more official provider integrations in the future. Furthermore, developers can create their own custom providers by implementing the `LLMProvider` interface to connect VoltAgent to virtually any AI model or service.

```typescript
// 1. Vercel AI Provider (integrates with various models via Vercel AI SDK)
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai"; // Model definition for OpenAI via Vercel
import { anthropic } from "@ai-sdk/anthropic"; // Model definition for Anthropic via Vercel

// Agent using OpenAI via Vercel
const vercelOpenAIAgent = new Agent({
  name: "Vercel OpenAI Assistant",
  instructions: "Assistant using Vercel AI SDK with OpenAI.",
  llm: new VercelAIProvider(), // The provider
  model: openai("gpt-4o"), // The specific model
});

// Agent using Anthropic via Vercel
const vercelAnthropicAgent = new Agent({
  name: "Vercel Anthropic Assistant",
  instructions: "Assistant using Vercel AI SDK with Anthropic.",
  llm: new VercelAIProvider(), // Same provider
  model: anthropic("claude-3-5-sonnet-20240620"), // Different model
});

// 2. XsAI Provider (Example of a custom/alternative provider)
import { XsAIProvider } from "@voltagent/xsai";

// Agent using XsAI Provider (might use different model naming)
const xsaiAgent = new Agent({
  name: "XsAI Assistant",
  instructions: "Assistant using XsAI Provider.",
  llm: new XsAIProvider({ apiKey: process.env.OPENAI_API_KEY }), // Provider instance
  model: "xsai-model-name", // Model identifier specific to this provider
});

// Use the agents (example)
const response = await vercelOpenAIAgent.generateText("Hello OpenAI via Vercel!");
console.log(response.text);

const response2 = await xsaiAgent.generateText("Hello XsAI!");
console.log(response2.text);
```

__Provider Options__ __Why?__ To provide a standardized way to configure model behavior across different LLM providers, making it ea asier to adjust generation parameters without worrying about provider-specific implementation details.

VoltAgent uses a standardized `ProviderOptions` type that abstracts common LLM configuration options like tempe erature, max tokens, and frequency penalties. These options are automatically mapped to each provider's specific c format internally, giving you a consistent developer experience regardless of which provider you're using.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  name: "Configurable Assistant",
  instructions: "An assistant with configurable generation parameters",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
});

// Example: Configure common LLM parameters regardless of provider
const response = await agent.generateText("Write a creative story about a robot.", {
  provider: {
    // Fine-tune generation behavior with standardized options
    temperature: 0.8, // Higher creativity (0-1)
    maxTokens: 500, // Limit response length
    topP: 0.9, // Nucleus sampling parameter
    frequencyPenalty: 0.5, // Reduce repetition
    presencePenalty: 0.3, // Encourage topic diversity
    seed: 12345, // Reproducible results
    stopSequences: ["THE END"], // Stop generation at specific string

    // Add provider callbacks for streaming
    onStepFinish: async (step) => {
      console.log("Step complete:", step.type);
    },
    onFinish: async (result) => {
      console.log("Generation complete!");
    },
    onError: async (error) => {
      console.error("Generation error:", error);
    },

    // Provider-specific options not covered by standard fields
    extraOptions: {
      someProviderSpecificOption: "value",
    },
  },
});

// Alternative: Provide parameters for streamed responses
const streamedResponse = await agent.streamText("Generate a business plan", {
  provider: {
    temperature: 0.3, // More focused, less creative
    maxTokens: 2000, // Longer response limit
    // ... other options as needed
  },
});
```

Use these standardized options to:

- Fine-tune response characteristics (creativity, length, diversity)
- Register callbacks for streaming events
- Achieve consistent behavior across different LLM providers
- Create reproducible outputs with the same seed value The options are applied consistently whether you're using `generateText`, `streamText`, `generateObject`, or `s streamObject` methods.

__MCP (Model Context Protocol)__ __Why?__ To enable standardized communication between your agent and external, potentially independent, model/t tool servers, promoting interoperability and modular deployment.

Connect to external servers that adhere to the MCP specification to leverage their capabilities (e.g., speciali ized models or tools) without directly integrating their code. MCP tools are treated like any other tool and can n be invoked during `generate*/stream*` calls.

```typescript
import { Agent, MCPConfiguration } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Set up MCP configuration pointing to your external server(s)
const mcpConfig = new MCPConfiguration({
  servers: {
    // Define one or more MCP-compliant servers
    myModelServer: {
      type: "http", // Communication type
      url: "https://my-mcp-server.example.com", // URL of the MCP server
    },
  },
});

// Asynchronously fetch tools offered by the configured MCP server(s)
const mcpTools = await mcpConfig.getTools();

// Create an agent that can utilize these external MCP tools
const agent = new Agent({
  name: "MCP Agent",
  instructions: "Uses external model capabilities via MCP",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  // Add the tools fetched from the MCP server
  tools: mcpTools,
});

// Example: Call streamText
const response = await agent.generateText("Use the external analysis tool on this data...");
console.log(response.text);
// The agent can now potentially call tools hosted on 'myModelServer'.
```

__Voice__ __Why?__ To build voice-based applications by adding speech-to-text (STT) and text-to-speech (TTS) capabilities s to your agent.

Integrate with voice providers like OpenAI or ElevenLabs. Use the provider directly for STT/TTS, or configure i it on the agent (`agent.voice`) and use its methods (e.g., `agent.voice.speak()`) to synthesize the agent's text t response.

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Import voice providers
import { OpenAIVoiceProvider, ElevenLabsVoiceProvider } from "@voltagent/voice";
import { createReadStream, createWriteStream } from "fs";
import { pipeline } from "stream/promises";

// --- Using a Voice Provider directly ---
// Option 1: OpenAI Voice
const openaiVoice = new OpenAIVoiceProvider({
  apiKey: process.env.OPENAI_API_KEY,
  ttsModel: "tts-1", // Text-to-Speech model
  voice: "alloy", // Choose a voice (alloy, echo, fable, onyx, nova, shimmer)
});

// Text to Speech (TTS) -> Returns a Readable stream of audio data
const audioStream = await openaiVoice.speak("Hello from OpenAI voice!");

// Example: Pipe the audio stream to a file
await pipeline(audioStream, createWriteStream("openai_output.mp3"));

// Speech to Text (STT) -> Takes an audio source (e.g., Readable stream)
const audioFileStream = createReadStream("input.mp3");
const transcript = await openaiVoice.listen(audioFileStream);
console.log("OpenAI Transcript:", transcript);

// Option 2: ElevenLabs Voice
const elevenLabsVoice = new ElevenLabsVoiceProvider({
  apiKey: process.env.ELEVENLABS_API_KEY,
  voice: "Rachel", // Choose an ElevenLabs voice ID or name
});

// TTS with ElevenLabs
const elAudioStream = await elevenLabsVoice.speak("Hello from ElevenLabs!");
await pipeline(elAudioStream, createWriteStream("elevenlabs_output.mp3"));

// --- Integrating Voice with an Agent ---
const agent = new Agent({
  name: "Voice Assistant",
  instructions: "A helpful voice assistant",
  llm: new VercelAIProvider(),
  model: openai("gpt-4o"),
  // Assign a voice provider instance to the agent's voice property
  voice: elevenLabsVoice, // Or use openaiVoice
});

// To generate voice from an agent response:
// 1. Generate the text response using a core agent method.
const textResponse = await agent.generateText("Tell me a short story.");

// 2. Check if the agent has a voice provider configured.
if (agent.voice && textResponse.text) {
  // 3. Call the 'speak' method on the agent's voice provider instance.
  console.log("Generating voice output...");
  const agentAudioStream = await agent.voice.speak(textResponse.text);

  // Example: Save the agent's spoken response to a file
  await pipeline(agentAudioStream, createWriteStream("agent_story.mp3"));
  console.log("Generated voice output stream.");
} else {
  console.log("Agent response:", textResponse.text);
  if (!agent.voice) {
    console.log("(Agent has no voice provider configured)");
  }
}
```

__Error Handling__ When interacting with agents (`generateText`, `streamText`, etc.), operations can fail due to network issues, A API errors, tool execution problems, or other runtime exceptions.

__Synchronous Errors (e.g., during setup):__ Use standard JavaScript `try...catch` blocks around the agent method calls (`generateText`, `streamText`, `gene erateObject`, `streamObject`). This will catch errors that occur *before* the main operation or stream begins, s such as configuration issues or initial API connection failures.

```typescript
const agent = new Agent({
  /* ... configuration ... */
});

try {
  // This try/catch handles errors during the initial call setup
  const response = await agent.streamText("Some complex request that might fail initially");

  // Processing the stream itself might encounter errors handled differently (see below)
  console.log("Stream processing started...");
  for await (const delta of response.stream) {
    // ... handle deltas ...
    process.stdout.write(delta.type === "text-delta" ? delta.textDelta : "");
  }
  // Note: If an error occurs *during* the stream, the loop might finish,
  // but the final history entry status will indicate an error.
  console.log("Interaction finished processing stream.");
} catch (error) {
  // Catches errors from the initial await agent.streamText() call
  console.error("Agent interaction failed during setup:", error);
  // Implement fallback logic, inform the user, or log the error
}
```

__Asynchronous Errors (e.g., during streaming):__ Errors that occur *during* the streaming process (after the initial `await agent.streamText()` call succeeds) a are handled internally by VoltAgent:

- The corresponding history entry is automatically updated with an error status.
- An error event is added to the agent's timeline.
- These errors __do not__ typically cause the `await agent.streamText(...)` call or the `for await...of respo onse.stream` loop itself to throw.

To observe or react to these asynchronous errors, you can:

- __Check History:__ After the stream finishes (the `for await` loop completes), check the status of the corr responding `AgentHistoryEntry`.
- __Use Agent Hooks:__ The existing hooks (`onStart`, `onEnd`, `onToolStart`, `onToolEnd`) can still provide valuable context for logging and debugging around the points where errors might occur, even though there isn't a specific `onError` hook.
- __Use `onError` Callback (Per-Call):__ Pass an `onError` callback directly in the provider options when cal lling `streamText` (or other methods). This is the most direct way to be notified of errors *during* the stream for a specific call.

```typescript
// Example with streamText
const response = await agent.streamText("Another request", {
  provider: {
    onError: async (error) => {
      console.error("onError callback: Stream encountered an error:", error);
      // Implement specific error handling for this call
    },
  },
});
```

By combining `try...catch` for initial errors and using the per-call `onError` callback or checking history for r stream errors, you can effectively manage issues during agent interactions.

---

### createPrompt Utility

The `createPrompt` utility is used to define dynamic prompt templates for VoltAgent agents. This allows for fle exible and context-aware instructions for the LLM, making prompts more reusable and adaptable to different scena arios.

__Dynamic Prompts with `createPrompt`__

```typescript
import { createPrompt } from "@voltagent/core";

const agentPrompt = createPrompt({
  template: `You are a specialist in {{domain}}.

Current Context: {{context}}
Task Type: {{task_type}}

{{strategy}}

Always use 'think' to analyze before proceeding.`,
  variables: {
    domain: "your specialty",
    context: "general context",
    task_type: "standard",
    strategy: "Your approach strategy"
  }
});
```

When creating an agent, you can pass the result of `createPrompt()` to its `instructions` property:

```typescript
import { Agent } from "@voltagent/core";
import { VercelAIProvider } from "@voltagent/vercel-ai";
import { google } from "../config/googleProvider"; // Assuming this exports the google model

// ... (define agentPrompt using createPrompt as above)

export const someAgent = new Agent({
  name: "SomeAgent",
  purpose: "To perform specific tasks within its domain.",
  description: "A specialized agent with dynamic instructions.",
  instructions: agentPrompt(), // Use the created dynamic prompt here
  llm: new VercelAIProvider(),
  model: google("gemini-2.5-flash-lite-preview-06-17"),
  // ... other configurations
});
```

This allows the agent's behavior to be dynamically influenced by the variables provided to the prompt template, , making the agent more adaptable without hardcoding its core instructions. It is particularly useful for settin ng up domain-specific prompts for sub-agents or for supervisors to dynamically adjust their delegation strategy.

---

__Most Relevant Files:__

- `.clinerules/voltAgent.instructions.md`
- `.github/instructions/voltAgent.instructions.md`
- `.windsurf/rules/voltagent.md`
- `voltagent/GEMINI.md`
- The user-provided context documents for "Subagents | VoltAgent", "Hooks | VoltAgent", and "Overview | VoltA Agent" were also directly used as primary sources. While not files within the repository, they contain VoltAgent t documentation directly relevant to the query.

:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe
